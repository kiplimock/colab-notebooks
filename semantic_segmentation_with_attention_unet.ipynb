{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCGz3kBr90XJZKJidRcVtv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kiplimock/colab-notebooks/blob/main/semantic_segmentation_with_attention_unet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention Augemented U-Net for Tree Crown Segmentation\n",
        "\n",
        "Based on the following papers:\n",
        "\n",
        "1. Jodas, D. S., Velasco, G. D. N., de Lima, R. A., Machado, A. R., & Papa, J. P. (2023). Deep Learning Semantic Segmentation Models for Detecting the Tree Crown Foliage. In VISIGRAPP (4: VISAPP) (pp. 143-150). https://www.scitepress.org/PublishedPapers/2023/116046/116046.pdf\n",
        "\n",
        "2. Woo, S., Park, J., Lee, JY., Kweon, I.S. (2018). CBAM: Convolutional Block Attention Module. In: Ferrari, V., Hebert, M., Sminchisescu, C., Weiss, Y. (eds) Computer Vision - ECCV 2018. ECCV 2018. Lecture Notes in Computer Science(), vol 11211. Springer, Cham. https://doi.org/10.1007/978-3-030-01234-2_1"
      ],
      "metadata": {
        "id": "wA-RpUrmavVC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Architecture\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://camo.githubusercontent.com/c40a3febddbb349098cf67e237a46f09489a098907772edc30619877f2980039/68747470733a2f2f64726976652e676f6f676c652e636f6d2f75633f6578706f72743d766965772669643d31307532665a6c2d4f4761364a45435f433852493038576933484356574f727057\" alt=\"model architecture\">\n",
        "</p>"
      ],
      "metadata": {
        "id": "kbyn5iMycJTy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CBAM Module Architecture\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media.springernature.com/full/springer-static/image/chp%3A10.1007%2F978-3-030-01234-2_1/MediaObjects/474212_1_En_1_Fig1_HTML.gif?as=webp\" alt=\"cbam architecture\">\n",
        "</p>\n",
        "\n",
        "### Submodules of CBAM\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media.springernature.com/full/springer-static/image/chp%3A10.1007%2F978-3-030-01234-2_1/MediaObjects/474212_1_En_1_Fig2_HTML.gif?as=webp\" alt=\"cbam submodules\">\n",
        "</p>"
      ],
      "metadata": {
        "id": "Higb0t6WdVgy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "m-nyfcFOc892"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, Reshape, Dense, Input\n",
        "from tensorflow.keras.layers import Activation, Concatenate, Conv2D, Multiply"
      ],
      "metadata": {
        "id": "D2aD5UG9c_Eu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conv2D Block"
      ],
      "metadata": {
        "id": "LmCePTdNkfGY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QbdcHIz5kefF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention Module"
      ],
      "metadata": {
        "id": "kZaM9eJ1cQJS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LHNSoJDMasI9"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Implementation of CBAM: Convolutional Block Attention Module in the TensorFlow 2.5.\n",
        "Paper: https://arxiv.org/pdf/1807.06521\n",
        "Code: https://github.com/nikhilroxtomar/Attention-Mechanism-Implementation/blob/main/TensorFlow/cbam.py\n",
        "\"\"\"\n",
        "\n",
        "def channel_attention_module(x, ratio=8):\n",
        "    batch, _, _, channel = x.shape\n",
        "\n",
        "    ## Shared layers\n",
        "    l1 = Dense(channel//ratio, activation=\"relu\", use_bias=False)\n",
        "    l2 = Dense(channel, use_bias=False)\n",
        "\n",
        "    ## Global Average Pooling\n",
        "    x1 = GlobalAveragePooling2D()(x)\n",
        "    x1 = l1(x1)\n",
        "    x1 = l2(x1)\n",
        "\n",
        "    ## Global Max Pooling\n",
        "    x2 = GlobalMaxPooling2D()(x)\n",
        "    x2 = l1(x2)\n",
        "    x2 = l2(x2)\n",
        "\n",
        "    ## Add both the features and pass through sigmoid\n",
        "    feats = x1 + x2\n",
        "    feats = Activation(\"sigmoid\")(feats)\n",
        "    feats = Multiply()([x, feats])\n",
        "\n",
        "    return feats\n",
        "\n",
        "def spatial_attention_module(x):\n",
        "    ## Average Pooling\n",
        "    x1 = tf.reduce_mean(x, axis=-1)\n",
        "    x1 = tf.expand_dims(x1, axis=-1)\n",
        "\n",
        "    ## Max Pooling\n",
        "    x2 = tf.reduce_max(x, axis=-1)\n",
        "    x2 = tf.expand_dims(x2, axis=-1)\n",
        "\n",
        "    ## Concatenat both the features\n",
        "    feats = Concatenate()([x1, x2])\n",
        "    ## Conv layer\n",
        "    feats = Conv2D(1, kernel_size=7, padding=\"same\", activation=\"sigmoid\")(feats)\n",
        "    feats = Multiply()([x, feats])\n",
        "\n",
        "    return feats\n",
        "\n",
        "def cbam(x):\n",
        "    x = channel_attention_module(x)\n",
        "    x = spatial_attention_module(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = Input(shape=(128, 128, 32))\n",
        "cbam(inputs).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6843i7-fWhU",
        "outputId": "b4debfc3-8da3-4bf8-aa85-9c69a6b7a001"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([None, 128, 128, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}